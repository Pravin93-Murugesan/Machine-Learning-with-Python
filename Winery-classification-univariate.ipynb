{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Winery classification using the one-dimensional Gaussian"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The **Wine** data set contains 178 labeled data points, each corresponding to a bottle of wine:\n* The features (`x`): a 13-dimensional vector consisting of visual and chemical features for the bottle of wine\n* The label (`y`): the winery from which the bottle came (1,2 or 3)\n\nThe data can be downloaded from the UCI repository (https://archive.ics.uci.edu/ml/datasets/wine)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load in the data set"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We start by loading the packages we will need."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Useful module for dealing with the Gaussian density\nfrom scipy.stats import norm, multivariate_normal\n\n# installing packages for interactive graphs\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom ipywidgets import interact, interactive, fixed, interact_manual, IntSlider",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we load the Wine data set and divide these into a training set of 130 points and a test set of 48 points."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = np.loadtxt('wine.data.txt', delimiter=',')\n\n# Names of features\nfeaturenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n                'OD280/OD315 of diluted wines', 'Proline']",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Fix a particular \"random\" permutation of the data, and use these to effect the training / test split."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Split 178 instances into training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n# Also split apart data and labels\nnp.random.seed(0)\nperm = np.random.permutation(178)\ntrainx = data[perm[0:130],1:14]\ntrainy = data[perm[0:130],0]\ntestx = data[perm[130:178], 1:14]\ntesty = data[perm[130:178],0]",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We get four arrays:\n* `trainx`: 130x13, the training points\n* `trainy`: 130x1, labels of the training points\n* `testx`: 48x13, the test points\n* `testy`: 48x1, labels of the test points"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's see how many training points there are from each class."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sum(trainy==1), sum(trainy==2), sum(trainy==3)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "(43, 54, 33)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Look at the distribution of a single feature from one of the wineries"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's pick just one feature: 'Alcohol'. This is the first feature. Here is a *histogram* of this feature's values under class 1, along with the *Gaussian fit* to this distribution.\n\n<img src=\"histogram.png\">\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To generate a figure like this, the following function, **density_plot** does this for any feature and label. The first line adds an interactive component that lets you choose these parameters using sliders. "
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "@interact_manual( feature=IntSlider(0,0,12), label=IntSlider(0,1,3) )\ndef density_plot(feature, label):\n    plt.hist(trainx[trainy==label,feature], normed=True)\n    \n    mu = np.mean(trainx[trainy==label,feature]) # mean\n    var = np.var(trainx[trainy==label,feature]) # variance\n    std = np.sqrt(var) # standard deviation\n    \n    x_axis = np.linspace(mu - 3*std, mu + 3*std, 1000)\n    plt.plot(x_axis, norm.pdf(x_axis,mu,std), 'r', lw=3)\n    plt.title(\"Winery \"+ str(label) )\n    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n    plt.ylabel('Density', fontsize=14, color='red')\n    plt.show()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c51af98698c484a80e029ecc74145ff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT0wLCBkZXNjcmlwdGlvbj11J2ZlYXR1cmUnLCBtYXg9MTIpLCBJbnRTbGlkZXIodmFsdWU9MSwgZGVzY3JpcHRpb249dSdsYWLigKY=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Fit a Gaussian to each class"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's define a function that will fit a Gaussian generative model to the three classes, restricted to just a single feature."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Assumes y takes on values 1,2,3\ndef fit_generative_model(x,y,feature):\n    k = 3 # number of classes\n    mu = np.zeros(k+1) # list of means\n    var = np.zeros(k+1) # list of variances\n    pi = np.zeros(k+1) # list of class weights\n    for label in range(1,k+1):\n        indices = (y==label)\n        mu[label] = np.mean(x[indices,feature])\n        var[label] = np.var(x[indices,feature])\n        pi[label] = float(sum(indices))/float(len(y))\n    return mu, var, pi",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Call this function on the feature 'alcohol'. What are the class weights?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "feature = 0 # 'alcohol'\nmu, var, pi = fit_generative_model(trainx, trainy, feature)\nprint pi[1:]",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[0.33076923 0.41538462 0.25384615]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, display the Gaussian distribution for each of the three classes"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "@interact_manual( feature=IntSlider(0,0,12) )\ndef show_densities(feature):\n    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n    colors = ['r', 'k', 'g']\n    \n    for label in range(1,4):\n        m = mu[label]\n        s = np.sqrt(var[label])\n        x_axis = np.linspace(m - 3*s, m+3*s, 1000)\n        plt.plot(x_axis, norm.pdf(x_axis,m,s), colors[label-1], label=\"class \" + str(label))\n        \n    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n    plt.ylabel('Density', fontsize=14, color='red')\n    plt.legend()\n    plt.show()",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2503a798b83648cdb22065ce4f8f31b8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT0wLCBkZXNjcmlwdGlvbj11J2ZlYXR1cmUnLCBtYXg9MTIpLCBCdXR0b24oZGVzY3JpcHRpb249dSdSdW4gSW50ZXJhY3QnLCDigKY=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Predict labels for the test set"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "How well can we predict the class (1,2,3) based just on one feature? The code below lets us find this out."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "@interact( feature=IntSlider(0,0,12) )\ndef test_model(feature):\n    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n\n    k = 3 # Labels \n    n_test = len(testy) # Number of test points\n    score = np.zeros((n_test,k+1))\n    \n    for i in range(0,n_test):\n        for label in range(1,k+1):\n            score[i,label] = (pi[label]) * norm.pdf(testx[i,feature], mu[label], np.sqrt(var[label])) # or (np.log... + norm.logpdf...)\n            \n    predictions = np.argmax(score[:,], axis=1)\n    \n    errors = np.sum(predictions != testy)\n    print \"Test error using feature \" + featurenames[feature] + \": \" + str(errors) + \"/\" + str(n_test)\n    \n    return round(float(errors)/n_test, 3)",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5733f927c2ad4b36945132ca48911a81",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT0wLCBkZXNjcmlwdGlvbj11J2ZlYXR1cmUnLCBtYXg9MTIpLCBPdXRwdXQoKSksIF9kb21fY2xhc3Nlcz0odSd3aWRnZXQtaW7igKY=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Feature selection"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this notebook, we are looking at classifiers that use just one out of a possible 13 features. Choosing a subset of features is called `feature selection`. In general, this is something we would need to do based solely on the *training set*--that is, without peeking at the *test set*.\n\nFor the wine data, we compute the test error associated with each choice of feature."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Test error\ntest_error = np.zeros(13)\nfor feature in range(0,13):\n    test_error[feature] = test_model(feature)\ntest_error",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Test error using feature Alcohol: 17/48\nTest error using feature Malic acid: 23/48\nTest error using feature Ash: 29/48\nTest error using feature Alcalinity of ash: 23/48\nTest error using feature Magnesium: 21/48\nTest error using feature Total phenols: 16/48\nTest error using feature Flavanoids: 8/48\nTest error using feature Nonflavanoid phenols: 23/48\nTest error using feature Proanthocyanins: 16/48\nTest error using feature Color intensity: 10/48\nTest error using feature Hue: 14/48\nTest error using feature OD280/OD315 of diluted wines: 19/48\nTest error using feature Proline: 17/48\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "array([0.354, 0.479, 0.604, 0.479, 0.438, 0.333, 0.167, 0.479, 0.333,\n       0.208, 0.292, 0.396, 0.354])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Choosing feature `Flavanoids` results in the lowest test error."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": false,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}