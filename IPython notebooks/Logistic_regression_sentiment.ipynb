{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Sentiment analysis using logistic regression"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The Sentiment data set consists of 3000 sentences which come from reviews on `imdb.com`, `amazon.com`, and `yelp.com`. Each sentence is labeled according to whether it comes from a positive review or negative review.\n\nWe will use `logistic regression` to learn a classifier from this data."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Set up notebook, load and preprocess data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, some standard includes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n%matplotlib inline\nimport string\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rc('xtick', labelsize=14) \nmatplotlib.rc('ytick', labelsize=14)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The data set consists of 3000 sentences, each labeled '1' (if it came from a positive review) or '0' (if it came from a negative review). We will change the negative review label to '-1'."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Read in the data set\nwith open(\"full_set.txt\") as f:\n    content = f.readlines()\n\n## Remove leading and trailing white space\ncontent = [x.strip() for x in content]\n\n## Separate the sentences from the labels\nsentences = [x.split(\"\\t\")[0] for x in content]\nlabels = [x.split(\"\\t\")[1] for x in content]\n\n## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\ny = np.array(labels, dtype='int8')\ny = 2*y - 1",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Preprocessing the text data\n\nTo transform this prediction problem into one amenable to linear classification, we will first need to preprocess the text data. We will do four transformations:\n\n1. Remove punctuation and numbers.\n2. Transform all words to lower-case.\n3. Remove _stop words_.\n4. Convert the sentences into vectors, using a bag-of-words representation.\n\nWe begin with first two steps."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## full_remove takes a string x and a list of characters removal_list \n## returns x with all the characters in removal_list replaced by ' '\ndef full_remove(x, removal_list):\n    for w in removal_list:\n        x = x.replace(w, ' ')\n    return x\n\n## Remove digits\ndigits = [str(x) for x in range(10)]\ndigit_less = [full_remove(x, digits) for x in sentences]\n\n## Remove punctuation\npunc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n\n## Make everything lower-case\nsents_lower = [x.lower() for x in punc_less]",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Stop words\n\nStop words are words that are filtered out because they are believed to contain no useful information for the task at hand. These usually include articles such as 'a' and 'the', pronouns such as 'i' and 'they', and prepositions such 'to' and 'from'. We have put together a very small list of stop words, but these are by no means comprehensive. Feel free to use something different; for instance, larger lists can easily be found on the web."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Run these two lines just once and download 'stopwords' from the interactive menu.\n* import nltk \n* nltk.download()"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Define our stop words\nfrom nltk.corpus import stopwords\nstop_set = set(stopwords.words('english'))\n\n## Remove stop words\nsents_split = [x.split() for x in sents_lower] # example - ['the', 'mic', 'is', 'great']\nsents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split] # example - 'mic great'",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let us see how the sentences look so far."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sents_processed[0:10]",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "['way plug us unless go converter',\n 'good case excellent value',\n 'great jawbone',\n 'tied charger conversations lasting minutes major problems',\n 'mic great',\n 'jiggle plug get line right get decent volume',\n 'several dozen several hundred contacts imagine fun sending one one',\n 'razr owner must',\n 'needless say wasted money',\n 'waste money time']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Bag of words\n\nIn order to use linear classifiers on our data set, we need to transform our textual data into numeric data. The classical way to do this is known as the `bag of words` representation. \n\nIn this representation, each word is thought of as corresponding to a number in `{1, 2, ..., V}` where `V` is the size of our vocabulary. And each sentence is represented as a V-dimensional vector $x$, where $x_i$ is the number of times that word $i$ occurs in the sentence.\n\nTo do this transformation, we will make use of the `CountVectorizer` class in `scikit-learn`. We will cap the number of features at 4500, meaning a word will make it into our vocabulary only if it is one of the 4500 most common words in the corpus. This is often a useful step as it can weed out spelling mistakes and words which occur too infrequently to be useful."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# See Sebastian Raschka - Python Machine Learning, page 236/237 for reference\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n## Transform to bag of words representation.\nvectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\ndata_features = vectorizer.fit_transform(sents_processed)\n\ndata_mat = data_features.toarray()",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Training / test split\n\nFinally, we split the data into a training set of 2500 sentences and a test set of 500 sentences (of which 250 are positive and 250 negative)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Split the data into training and testingsets\nnp.random.seed(0)\ntest_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\ntrain_inds = list(set(range(len(labels))) - set(test_inds))\n\ntrain_data = data_mat[train_inds,]\ntrain_labels = y[train_inds]\n\ntest_data = data_mat[test_inds,]\ntest_labels = y[test_inds]\n\nprint\"Train data: \", train_data.shape\nprint\"Test data: \", test_data.shape",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train data:  (2500, 4500)\nTest data:  (500, 4500)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Fitting a logistic regression model to the training data\n\nWe could implement our own logistic regression solver using stochastic gradient descent, but fortunately, there is already one built into `scikit-learn`.\n\nDue to the randomness in the SGD procedure, different runs can yield slightly different solutions (and thus different error values)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import SGDClassifier\n\n## Fit logistic classifier on training data\nclf = SGDClassifier(loss=\"log\", penalty=\"none\")\nclf.fit(train_data, train_labels)\n\n## Pull out the parameters (w,b) of the logistic regression model\nw = clf.coef_[0,:]\nb = clf.intercept_\n\n## Get predictions on training and test data\npreds_train = clf.predict(train_data)\npreds_test = clf.predict(test_data)\n\n## Compute errors\nerrs_train = np.sum(preds_train != train_labels)\nerrs_test = np.sum(preds_test != test_labels)\n\nprint\"Training error: \", float(errs_train)/len(train_labels)\nprint\"Test error: \", float(errs_test)/len(test_labels)\n\n## Compute accuracy\nprint'Test Accuracy: %.3f' % clf.score(test_data, test_labels)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training error:  0.0276\nTest error:  0.182\nTest Accuracy: 0.818\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "The results reveal that our machine learning model can predict whether a movie review is positive or negative ``with 81 percent accuracy``."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Words with large influence\n\nFinally, we attempt to partially interpret the logistic regression model.\n\nWhich words are most important in deciding whether a sentence is positive? As a first approximation to this, we simply take the words whose coefficients in `w` that have the largest positive values.\n\nLikewise, we look at the words whose coefficients in `w` that have the most negative values, and we think of these as influential in negative predictions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Convert vocabulary into a list:\nvocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n\n## Get indices of sorting w\ninds = np.argsort(w) # parameters (w,b) of the logistic regression model\n\n## Words with large positive values\npos_inds = inds[-49:-1]\nprint(\"Highly positive words: \\n \")\nprint([str(x) for x in list(vocab[pos_inds])])\nprint(\" \\n \")\n\n## Words with large negative values\nneg_inds = inds[0:50]\nprint(\"Highly negative words: \\n  \")\nprint([str(x) for x in list(vocab[neg_inds])])",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Highly positive words: \n \n['huston', 'white', 'world', 'blackberry', 'ladies', 'order', 'phenomenal', 'flavorful', 'happy', 'brilliant', 'soul', 'feature', 'assure', 'motorola', 'gem', 'slim', 'comfortable', 'amount', 'without', 'bowl', 'fantastic', 'sturdy', 'performance', 'exactly', 'keyboard', 'pleased', 'hand', 'great', 'cool', 'mouth', 'clear', 'data', 'wonderful', 'massive', 'perfect', 'liked', 'love', 'joy', 'works', 'delicious', 'inside', 'art', 'enjoyed', 'nice', 'incredible', 'awesome', 'loved', 'interesting']\n \n \nHighly negative words: \n  \n['poor', 'beep', 'pm', 'avoid', 'bland', 'ok', 'unfortunately', 'mediocre', 'fly', 'stupid', 'slow', 'make', 'wasted', 'sucks', 'storyline', 'racial', 'rude', 'worst', 'junk', 'dirty', 'fat', 'lacks', 'wife', 'waste', 'script', 'disappointment', 'cheap', 'failed', 'att', 'mistake', 'luck', 'guess', 'empty', 'joke', 'flat', 'cheesy', 'bye', 'fails', 'none', 'ridiculous', 'puppets', 'plug', 'average', 'garbage', 'crap', 'card', 'cover', 'clip', 'par', 'sugary']\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": false,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}