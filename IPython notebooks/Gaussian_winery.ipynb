{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Winery classification using Univariate / Bivariate / Multivariate Gaussian"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 1. Winery classification using the one-dimensional Gaussian"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The **Wine** data set contains 178 labeled data points, each corresponding to a bottle of wine:\n* The features (`x`): a 13-dimensional vector consisting of visual and chemical features for the bottle of wine\n* The label (`y`): the winery from which the bottle came (1,2 or 3)\n\nThe data can be downloaded from the UCI repository (https://archive.ics.uci.edu/ml/datasets/wine)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load in the data set"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We start by loading the packages we will need."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Useful module for dealing with the Gaussian density\nfrom scipy.stats import norm, multivariate_normal\n\n# installing packages for interactive graphs\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom ipywidgets import interact, interactive, fixed, interact_manual, IntSlider",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we load the Wine data set and divide these into a training set of 130 points and a test set of 48 points."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = np.loadtxt('wine.data.txt', delimiter=',')\n\n# Names of features\nfeaturenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n                'OD280/OD315 of diluted wines', 'Proline']",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Fix a particular \"random\" permutation of the data, and use these to effect the training / test split."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Split 178 instances into training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n# Also split apart data and labels\nnp.random.seed(0)\nperm = np.random.permutation(178)\ntrainx = data[perm[0:130],1:14]\ntrainy = data[perm[0:130],0]\ntestx = data[perm[130:178], 1:14]\ntesty = data[perm[130:178],0]",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We get four arrays:\n* `trainx`: 130x13, the training points\n* `trainy`: 130x1, labels of the training points\n* `testx`: 48x13, the test points\n* `testy`: 48x1, labels of the test points"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's see how many training points there are from each class."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sum(trainy==1), sum(trainy==2), sum(trainy==3)",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 34,
          "data": {
            "text/plain": "(43, 54, 33)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Look at the distribution of a single feature from one of the wineries"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's pick just one feature: 'Alcohol'. This is the first feature. Here is a *histogram* of this feature's values under class 1, along with the *Gaussian fit* to this distribution.\n\n<img src=\"histogram.png\">\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To generate a figure like this, the following function, **density_plot** does this for any feature and label. The first line adds an interactive component that lets you choose these parameters using sliders. "
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "@interact_manual( feature=IntSlider(0,0,12), label=IntSlider(0,1,3) )\ndef density_plot(feature, label):\n    plt.hist(trainx[trainy==label,feature], normed=True)\n    \n    mu = np.mean(trainx[trainy==label,feature]) # mean\n    var = np.var(trainx[trainy==label,feature]) # variance\n    std = np.sqrt(var) # standard deviation\n    \n    x_axis = np.linspace(mu - 3*std, mu + 3*std, 1000)\n    plt.plot(x_axis, norm.pdf(x_axis,mu,std), 'r', lw=3)\n    plt.title(\"Winery \"+ str(label) )\n    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n    plt.ylabel('Density', fontsize=14, color='red')\n    plt.show()",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cd6cb56d40b4783bbb436457001b765",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT0wLCBkZXNjcmlwdGlvbj11J2ZlYXR1cmUnLCBtYXg9MTIpLCBJbnRTbGlkZXIodmFsdWU9MSwgZGVzY3JpcHRpb249dSdsYWLigKY=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Fit a Gaussian to each class"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's define a function that will fit a Gaussian generative model to the three classes, restricted to just a single feature."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Assumes y takes on values 1,2,3\ndef fit_generative_model(x,y,feature):\n    k = 3 # number of classes\n    mu = np.zeros(k+1) # list of means\n    var = np.zeros(k+1) # list of variances\n    pi = np.zeros(k+1) # list of class weights\n    for label in range(1,k+1):\n        indices = (y==label)\n        mu[label] = np.mean(x[indices,feature])\n        var[label] = np.var(x[indices,feature])\n        pi[label] = float(sum(indices))/float(len(y))\n    return mu, var, pi",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Call this function on the feature 'alcohol'. What are the class weights?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "feature = 0 # 'alcohol'\nmu, var, pi = fit_generative_model(trainx, trainy, feature)\nprint pi[1:]",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[0.33076923 0.41538462 0.25384615]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, display the Gaussian distribution for each of the three classes"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "@interact_manual( feature=IntSlider(0,0,12) )\ndef show_densities(feature):\n    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n    colors = ['r', 'k', 'g']\n    \n    for label in range(1,4):\n        m = mu[label]\n        s = np.sqrt(var[label])\n        x_axis = np.linspace(m - 3*s, m+3*s, 1000)\n        plt.plot(x_axis, norm.pdf(x_axis,m,s), colors[label-1], label=\"class \" + str(label))\n        \n    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n    plt.ylabel('Density', fontsize=14, color='red')\n    plt.legend()\n    plt.show()",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c0a9c5934ad417c92b1a793a152933e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT0wLCBkZXNjcmlwdGlvbj11J2ZlYXR1cmUnLCBtYXg9MTIpLCBCdXR0b24oZGVzY3JpcHRpb249dSdSdW4gSW50ZXJhY3QnLCDigKY=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Predict labels for the test set"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "How well can we predict the class (1,2,3) based just on one feature? The code below lets us find this out."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "@interact( feature=IntSlider(0,0,12) )\ndef test_model(feature):\n    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n\n    k = 3 # Labels \n    n_test = len(testy) # Number of test points\n    score = np.zeros((n_test,k+1))\n    \n    for i in range(0,n_test):\n        for label in range(1,k+1):\n            score[i,label] = np.log(pi[label]) + norm.logpdf(testx[i,feature], mu[label], np.sqrt(var[label])) # using log is advantageous!\n            \n    predictions = np.argmax(score[:,1:4], axis=1) + 1\n    \n    errors = np.sum(predictions != testy)\n    print \"Test error using feature \" + featurenames[feature] + \": \" + str(errors) + \"/\" + str(n_test)\n    \n    return round(float(errors)/n_test, 3)",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4acb7e38912a4af884a134ac1dc6c244",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT0wLCBkZXNjcmlwdGlvbj11J2ZlYXR1cmUnLCBtYXg9MTIpLCBPdXRwdXQoKSksIF9kb21fY2xhc3Nlcz0odSd3aWRnZXQtaW7igKY=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Feature selection"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this notebook, we are looking at classifiers that use just one out of a possible 13 features. Choosing a subset of features is called `feature selection`. In general, this is something we would need to do based solely on the *training set*--that is, without peeking at the *test set*.\n\nFor the wine data, we compute the test error associated with each choice of feature."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Test error\ntest_error = np.zeros(13)\nfor feature in range(0,13):\n    test_error[feature] = test_model(feature)\ntest_error",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Test error using feature Alcohol: 17/48\nTest error using feature Malic acid: 23/48\nTest error using feature Ash: 29/48\nTest error using feature Alcalinity of ash: 23/48\nTest error using feature Magnesium: 21/48\nTest error using feature Total phenols: 16/48\nTest error using feature Flavanoids: 8/48\nTest error using feature Nonflavanoid phenols: 23/48\nTest error using feature Proanthocyanins: 16/48\nTest error using feature Color intensity: 10/48\nTest error using feature Hue: 14/48\nTest error using feature OD280/OD315 of diluted wines: 19/48\nTest error using feature Proline: 17/48\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 47,
          "data": {
            "text/plain": "array([0.354, 0.479, 0.604, 0.479, 0.438, 0.333, 0.167, 0.479, 0.333,\n       0.208, 0.292, 0.396, 0.354])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Choosing feature `Flavanoids` results in the lowest test error."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 2. Winery classification using the two-dimensional Gaussian"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our first generative model for Winery classification used just one feature. Now we use two features, modeling each class by a **bivariate Gaussian**."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Fit a Gaussian to each class"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now define a function that will fit a Gaussian generative model to the three classes, restricted to a given list of features. The function returns:\n* `mu`: the means of the Gaussians, one per row\n* `covar`: covariance matrices of each of the Gaussians\n* `pi`: list of three class weights summing to 1"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Fit a Gaussian to a data set using the selected features\ndef fit_gaussian2(x, features):\n    mu = np.mean(x[:,features], axis=0)\n    covar = np.cov(x[:,features], rowvar=0, bias=1)\n    return mu, covar",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Assumes y takes on values 1,2,3\ndef fit_generative_model2(x, y, features):\n    k = 3 # number of classes\n    d = len(features) # number of features\n    mu = np.zeros((k+1,d)) # list of means\n    covar = np.zeros((k+1,d,d)) # list of covariance matrices\n    pi = np.zeros(k+1) # list of class weights\n    for label in range(1,k+1):\n        indices = (y==label)\n        mu[label,:], covar[label,:,:] = fit_gaussian2(x[indices,:], features)\n        pi[label] = float(sum(indices))/float(len(y))\n    return mu, covar, pi",
      "execution_count": 110,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Predict labels for the test points"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This testing procedure is analogous to what was developed in the 1-d case. Let us see how well we can predict the class (1,2,3) based just on these two features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Now test the performance of a predictor based on a subset of features\n@interact( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1) )\ndef test_model2(f1, f2):\n    if f1 == f2: # need f1 != f2\n        print \"Please choose different features for f1 and f2.\"\n        return  \n    \n    features= [f1,f2]\n    mu, covar, pi = fit_generative_model2(trainx, trainy, features)\n    \n    k = 3 # Labels 1,2,...,k\n    nt = len(testy) # Number of test points\n    score = np.zeros((nt,k+1))\n    \n    for i in range(0,nt):\n        for label in range(1,k+1):\n            score[i,label] = np.log(pi[label]) + \\\n            multivariate_normal.logpdf(testx[i,features], mean=mu[label,:], cov=covar[label,:,:])\n            \n    predictions = np.argmax(score[:,1:4], axis=1) + 1\n    \n    # Finally, tally up score\n    errors = np.sum(predictions != testy)\n    print \"Test error using feature(s): \",\n        \n    for f in features:\n        print \"'\" + featurenames[f] + \"'\" + \" \",\n    print\n    print \"Errors: \" + str(errors) + \"/\" + str(nt) \n    \n    return round(float(errors)/nt, 3)",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d590202fde1f4e25b55d514882002c33",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "aW50ZXJhY3RpdmUoY2hpbGRyZW49KEludFNsaWRlcih2YWx1ZT0wLCBkZXNjcmlwdGlvbj11J2YxJywgbWF4PTEyKSwgSW50U2xpZGVyKHZhbHVlPTYsIGRlc2NyaXB0aW9uPXUnZjInLCBtYXjigKY=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Different pairs of features yield different test errors. Let us find out which pair of features achieves this minimum test error."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nf = len(testx[0])\nz = np.zeros((nf, nf))\nfor i in range(0, nf):\n        for j in range(0, nf):\n            if i == j: # need i != j\n                z[i,j] = 100 # a random high number\n                while (j!=12):\n                    j = j + 1\n            z[i,j] = test_model2(i, j)\nz[12,12] = 100\nprint(z)",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[1.00e+02 1.88e-01 2.50e-01 1.88e-01 2.92e-01 1.04e-01 8.30e-02 2.08e-01\n  1.67e-01 1.67e-01 1.25e-01 1.04e-01 1.88e-01]\n [1.88e-01 1.00e+02 4.58e-01 3.33e-01 2.29e-01 2.50e-01 1.88e-01 4.38e-01\n  2.92e-01 2.29e-01 3.33e-01 3.13e-01 1.88e-01]\n [2.50e-01 4.58e-01 1.00e+02 3.75e-01 4.79e-01 3.33e-01 2.08e-01 4.58e-01\n  4.38e-01 2.71e-01 2.71e-01 2.71e-01 3.13e-01]\n [1.88e-01 3.33e-01 3.75e-01 1.00e+02 2.29e-01 2.50e-01 1.67e-01 3.96e-01\n  4.17e-01 2.29e-01 1.46e-01 1.88e-01 2.71e-01]\n [2.92e-01 2.29e-01 4.79e-01 2.29e-01 1.00e+02 2.29e-01 1.46e-01 3.75e-01\n  1.88e-01 2.08e-01 1.46e-01 1.46e-01 2.92e-01]\n [1.04e-01 2.50e-01 3.33e-01 2.50e-01 2.29e-01 1.00e+02 1.46e-01 2.71e-01\n  3.13e-01 1.04e-01 1.67e-01 2.08e-01 1.88e-01]\n [8.30e-02 1.88e-01 2.08e-01 1.67e-01 1.46e-01 1.46e-01 1.00e+02 1.67e-01\n  1.67e-01 6.30e-02 8.30e-02 1.46e-01 8.30e-02]\n [2.08e-01 4.38e-01 4.58e-01 3.96e-01 3.75e-01 2.71e-01 1.67e-01 1.00e+02\n  3.75e-01 2.50e-01 2.92e-01 2.71e-01 2.29e-01]\n [1.67e-01 2.92e-01 4.38e-01 4.17e-01 1.88e-01 3.13e-01 1.67e-01 3.75e-01\n  1.00e+02 1.04e-01 2.29e-01 3.33e-01 2.08e-01]\n [1.67e-01 2.29e-01 2.71e-01 2.29e-01 2.08e-01 1.04e-01 6.30e-02 2.50e-01\n  1.04e-01 1.00e+02 1.67e-01 1.88e-01 1.04e-01]\n [1.25e-01 3.33e-01 2.71e-01 1.46e-01 1.46e-01 1.67e-01 8.30e-02 2.92e-01\n  2.29e-01 1.67e-01 1.00e+02 2.29e-01 1.46e-01]\n [1.04e-01 3.13e-01 2.71e-01 1.88e-01 1.46e-01 2.08e-01 1.46e-01 2.71e-01\n  3.33e-01 1.88e-01 2.29e-01 1.00e+02 8.30e-02]\n [1.88e-01 1.88e-01 3.13e-01 2.71e-01 2.92e-01 1.88e-01 8.30e-02 2.29e-01\n  2.08e-01 1.04e-01 1.46e-01 8.30e-02 1.00e+02]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ind = np.unravel_index(np.argmin(z, axis=None), z.shape)\nprint \"'\" + featurenames[ind[0]] + \"'\" + \" \" \"and\" + \" \" \"'\" + featurenames[ind[1]] + \"'\" + \" \" \"yield the lowest test error of\" + \" \" + str(z[ind])",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": "'Flavanoids' and 'Color intensity' yield the lowest test error of 0.063\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 3. Winery classification with the multivariate Gaussian"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, we do winery classification using the full set of 13 features."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Fit a Gaussian generative model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now define a function that fits a Gaussian generative model to the data.\nFor each class (`j=1,2,3`), we have:\n* `pi[j]`: the class weight\n* `mu[j,:]`: the mean, a 13-dimensional vector\n* `covar[j,:,:]`: the 13x13 covariance matrix\n\nThis means that `pi` is a 4x1 array (Python arrays are indexed starting at zero, and we aren't using `j=0`), `mu` is a 4x13 array and `covar` is a 4x13x13 array."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def fit_generative_model(x,y):\n    k = 3  # labels 1,2,...,k\n    d = (x.shape)[1]  # number of features\n    mu = np.zeros((k+1,d))\n    covar = np.zeros((k+1,d,d))\n    pi = np.zeros(k+1)\n    for label in range(1,k+1):\n        indices = (y == label)\n        mu[label] = np.mean(x[indices,:], axis=0)\n        covar[label] = np.cov(x[indices,:], rowvar=0, bias=1)\n        pi[label] = float(sum(indices))/float(len(y))\n    return mu, sigma, pi",
      "execution_count": 113,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Fit a Gaussian generative model to the training data\nmu, covar, pi = fit_generative_model(trainx,trainy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use the model to make predictions on the test set"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Now test the performance of a predictor based on a subset of features\ndef test_model(mu, covar, pi, features, testx, testy):\n    \n    k = 3 # Labels \n    n_test = len(testy) # Number of test points\n    score = np.zeros((n_test,k+1))\n    \n    for i in range(0,n_test):\n        for label in range(1,k+1):\n            score[i,label] = np.log(pi[label]) + \\\n            multivariate_normal.logpdf(testx[i,features], mean=mu[label,:], cov=covar[label,:,:])\n            \n    predictions = np.argmax(score[:,1:4], axis=1) + 1\n    \n    errors = np.sum(predictions != testy)\n    print \"Test error using all features: \" + str(errors) + \"/\" + str(n_test) \n    \n    return round(float(errors)/n_test, 3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "features = range(0, (trainx.shape)[1])\ntest_model(mu, covar, pi, features, testx, testy)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": false,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}